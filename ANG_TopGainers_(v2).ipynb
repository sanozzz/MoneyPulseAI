{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/1PRAEdVZX5rPd/zBb0B5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanozzz/MoneyPulseAI/blob/main/ANG_TopGainers_(v2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4PTbLK6zH3q",
        "outputId": "5aa4d0f7-6a3f-419d-b2f3-2e84116ccbcd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-19 01:44:17 - INFO - Logger initialized successfully with IST timezone.\n",
            "INFO:TopGainersLogger:Logger initialized successfully with IST timezone.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File logging initialized at: /content/TopGainers_2024-12-19.log\n",
            "Console logging initialized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-12-19 01:44:21 - INFO - OpenAI client initialized successfully.\n",
            "INFO:TopGainersLogger:OpenAI client initialized successfully.\n",
            "2024-12-19 01:44:21 - INFO - TopGainers class initialized successfully.\n",
            "INFO:TopGainersLogger:TopGainers class initialized successfully.\n",
            "2024-12-19 01:44:21 - INFO - Input file located at: /content/TopGainers.csv\n",
            "INFO:TopGainersLogger:Input file located at: /content/TopGainers.csv\n",
            "2024-12-19 01:44:21 - INFO - Starting the TopGainers process.\n",
            "INFO:TopGainersLogger:Starting the TopGainers process.\n",
            "2024-12-19 01:44:21 - INFO - Generating Contextual summaries.\n",
            "INFO:TopGainersLogger:Generating Contextual summaries.\n",
            "2024-12-19 01:44:34 - INFO - Perplexity summary generated for One Mobikwik Systems Ltd..\n",
            "INFO:TopGainersLogger:Perplexity summary generated for One Mobikwik Systems Ltd..\n",
            "2024-12-19 01:44:49 - INFO - Perplexity summary generated for Vishal Mega Mart Ltd..\n",
            "INFO:TopGainersLogger:Perplexity summary generated for Vishal Mega Mart Ltd..\n",
            "2024-12-19 01:45:26 - INFO - Perplexity summary generated for Craftsman Automation.\n",
            "INFO:TopGainersLogger:Perplexity summary generated for Craftsman Automation.\n",
            "2024-12-19 01:45:49 - INFO - Perplexity summary generated for Aurionpro Solutions.\n",
            "INFO:TopGainersLogger:Perplexity summary generated for Aurionpro Solutions.\n",
            "2024-12-19 01:45:49 - INFO - Fetching news from Perplexity.\n",
            "INFO:TopGainersLogger:Fetching news from Perplexity.\n",
            "2024-12-19 01:46:05 - INFO - Perplexity API response received for query: Find recent news and updates about Aurionpro Solutions in the last week.\n",
            "INFO:TopGainersLogger:Perplexity API response received for query: Find recent news and updates about Aurionpro Solutions in the last week.\n",
            "2024-12-19 01:46:11 - INFO - Perplexity API response received for query: Find recent news and updates about Craftsman Automation in the last week.\n",
            "INFO:TopGainersLogger:Perplexity API response received for query: Find recent news and updates about Craftsman Automation in the last week.\n",
            "2024-12-19 01:46:15 - INFO - Perplexity API response received for query: Find recent news and updates about One Mobikwik Systems Ltd. in the last week.\n",
            "INFO:TopGainersLogger:Perplexity API response received for query: Find recent news and updates about One Mobikwik Systems Ltd. in the last week.\n",
            "2024-12-19 01:46:18 - INFO - Perplexity API response received for query: Find recent news and updates about Vishal Mega Mart Ltd. in the last week.\n",
            "INFO:TopGainersLogger:Perplexity API response received for query: Find recent news and updates about Vishal Mega Mart Ltd. in the last week.\n",
            "2024-12-19 01:46:18 - INFO - Combining ChatGPT summaries and Perplexity news.\n",
            "INFO:TopGainersLogger:Combining ChatGPT summaries and Perplexity news.\n",
            "2024-12-19 01:46:38 - INFO - Combined summary successfully generated for One Mobikwik Systems Ltd..\n",
            "INFO:TopGainersLogger:Combined summary successfully generated for One Mobikwik Systems Ltd..\n",
            "2024-12-19 01:46:57 - INFO - Combined summary successfully generated for Vishal Mega Mart Ltd..\n",
            "INFO:TopGainersLogger:Combined summary successfully generated for Vishal Mega Mart Ltd..\n",
            "2024-12-19 01:47:21 - INFO - Combined summary successfully generated for Craftsman Automation.\n",
            "INFO:TopGainersLogger:Combined summary successfully generated for Craftsman Automation.\n",
            "2024-12-19 01:47:48 - INFO - Combined summary successfully generated for Aurionpro Solutions.\n",
            "INFO:TopGainersLogger:Combined summary successfully generated for Aurionpro Solutions.\n",
            "2024-12-19 01:47:48 - INFO - Generating final news byte for all stocks.\n",
            "INFO:TopGainersLogger:Generating final news byte for all stocks.\n",
            "2024-12-19 01:47:48 - INFO - Starting to generate the initial news byte.\n",
            "INFO:TopGainersLogger:Starting to generate the initial news byte.\n",
            "2024-12-19 01:48:30 - INFO - Initial news byte generated successfully.\n",
            "INFO:TopGainersLogger:Initial news byte generated successfully.\n",
            "2024-12-19 01:48:30 - INFO - Initial news byte generated with 530 words.\n",
            "INFO:TopGainersLogger:Initial news byte generated with 530 words.\n",
            "2024-12-19 01:48:30 - INFO - Initial news byte is within the word limit. Returning the final news byte.\n",
            "INFO:TopGainersLogger:Initial news byte is within the word limit. Returning the final news byte.\n",
            "2024-12-19 01:48:30 - INFO - Generating and saving plot.\n",
            "INFO:TopGainersLogger:Generating and saving plot.\n",
            "2024-12-19 01:48:31 - INFO - Mobile-friendly horizontal bar plot saved to /content/TopGainers.png\n",
            "INFO:TopGainersLogger:Mobile-friendly horizontal bar plot saved to /content/TopGainers.png\n",
            "2024-12-19 01:48:31 - INFO - Uploading the plot image to S3.\n",
            "INFO:TopGainersLogger:Uploading the plot image to S3.\n",
            "2024-12-19 01:48:41 - ERROR - Error generating or uploading plot: Failed to upload image to S3: Requesting secret aws_access_key_id timed out. Secrets can only be fetched when running from the Colab UI.\n",
            "ERROR:TopGainersLogger:Error generating or uploading plot: Failed to upload image to S3: Requesting secret aws_access_key_id timed out. Secrets can only be fetched when running from the Colab UI.\n",
            "2024-12-19 01:48:41 - INFO - Saving the final output CSV.\n",
            "INFO:TopGainersLogger:Saving the final output CSV.\n",
            "2024-12-19 01:48:41 - INFO - Output saved to /content/TopGainers_with_DetailedSummaries.csv\n",
            "INFO:TopGainersLogger:Output saved to /content/TopGainers_with_DetailedSummaries.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import boto3\n",
        "import requests\n",
        "from textwrap import fill\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import pytz\n",
        "import openai\n",
        "import time\n",
        "\n",
        "\n",
        "# Allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set up the logger globally\n",
        "\n",
        "\n",
        "def setup_logger():\n",
        "    \"\"\"Sets up a logger with both file and console handlers, formatted with IST timezone.\"\"\"\n",
        "    logger = logging.getLogger(\"TopGainersLogger\")\n",
        "\n",
        "    # Avoid adding duplicate handlers\n",
        "    if logger.hasHandlers():\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    # Set the logging level\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    # Define a custom formatter with IST timezone\n",
        "    class ISTFormatter(logging.Formatter):\n",
        "        def formatTime(self, record, datefmt=None):\n",
        "            ist = timezone(\"Asia/Kolkata\")\n",
        "            record_time = datetime.fromtimestamp(record.created, ist)\n",
        "            return record_time.strftime(datefmt or \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Create the formatter\n",
        "    formatter = ISTFormatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "    # File handler setup\n",
        "    try:\n",
        "        log_file = f\"/content/TopGainers_{datetime.now(timezone('Asia/Kolkata')).strftime('%Y-%m-%d')}.log\"\n",
        "        file_handler = logging.FileHandler(log_file, mode=\"a\")  # Append mode\n",
        "        file_handler.setLevel(logging.INFO)\n",
        "        file_handler.setFormatter(formatter)\n",
        "        logger.addHandler(file_handler)\n",
        "        print(f\"File logging initialized at: {log_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to set up file handler: {e}\")\n",
        "\n",
        "    # Stream handler setup (console logging)\n",
        "    try:\n",
        "        console_handler = logging.StreamHandler()\n",
        "        console_handler.setLevel(logging.INFO)\n",
        "        console_handler.setFormatter(formatter)\n",
        "        logger.addHandler(console_handler)\n",
        "        print(\"Console logging initialized.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to set up console handler: {e}\")\n",
        "\n",
        "    # Test the logger\n",
        "    logger.info(\"Logger initialized successfully with IST timezone.\")\n",
        "    return logger\n",
        "\n",
        "\n",
        "# Initialize the logger\n",
        "logger = setup_logger()\n",
        "\n",
        "\n",
        "\n",
        "def get_ist_timestamp():\n",
        "    \"\"\"Returns the current timestamp in IST.\"\"\"\n",
        "    ist = pytz.timezone('Asia/Kolkata')\n",
        "    return datetime.now(ist).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "\n",
        "class TopGainers:\n",
        "    def __init__(self, input_path='/content/TopGainers.csv', output_path='/content/TopGainers_with_DetailedSummaries.csv', plot_path='/content/TopGainers.png'):\n",
        "        \"\"\"\n",
        "        Initialize the TopGainers class with file paths and API keys for OpenAI and Perplexity.\n",
        "        \"\"\"\n",
        "        self.input_path = input_path\n",
        "        self.output_path = output_path\n",
        "        self.plot_path = plot_path\n",
        "\n",
        "        # Fetch API keys from Colab's userdata or environment variables\n",
        "        openai_api_key = userdata.get(\"OPENAI_API_KEY\") or os.environ.get(\"OPENAI_API_KEY\")\n",
        "        perplexity_api_key = userdata.get(\"PERPLEXITY_API_KEY\") or os.environ.get(\"PERPLEXITY_API_KEY\")\n",
        "\n",
        "        if not openai_api_key:\n",
        "            logger.error(\"OpenAI API key is not set. Please configure it in userdata or environment variables.\")\n",
        "            raise ValueError(\"OpenAI API key is missing.\")\n",
        "        if not perplexity_api_key:\n",
        "            logger.error(\"Perplexity API key is not set. Please configure it in userdata or environment variables.\")\n",
        "            raise ValueError(\"Perplexity API key is missing.\")\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        self.openai_api_key = openai_api_key\n",
        "        self.initialize_openai_client()\n",
        "\n",
        "        # Store the Perplexity API key for later use\n",
        "        self.perplexity_api_key = perplexity_api_key\n",
        "        self.api_url = \"https://api.perplexity.ai/chat/completions\"\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "\n",
        "        # Log successful initialization\n",
        "        logger.info(\"TopGainers class initialized successfully.\")\n",
        "\n",
        "        # Validate the input file path\n",
        "        self.validate_input_path()\n",
        "\n",
        "    def initialize_openai_client(self):\n",
        "        \"\"\"Initialize the OpenAI client with the API key.\"\"\"\n",
        "        openai.api_key = self.openai_api_key\n",
        "        self.client = openai\n",
        "        logger.info(\"OpenAI client initialized successfully.\")\n",
        "\n",
        "    def validate_input_path(self):\n",
        "        \"\"\"Validate the existence of the input file.\"\"\"\n",
        "        if not os.path.exists(self.input_path):\n",
        "            logger.error(f\"Input file not found at: {self.input_path}\")\n",
        "            raise FileNotFoundError(f\"Input file not found at: {self.input_path}\")\n",
        "        logger.info(f\"Input file located at: {self.input_path}\")\n",
        "\n",
        "\n",
        "    def generate_contextual_summary(self, row, max_retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Generate a conversational summary for a given stock row using Perplexity's API.\n",
        "        Retries the request in case of an error.\n",
        "        \"\"\"\n",
        "        # Determine stellar performance metrics\n",
        "        stellar_quarterly_performance = (\n",
        "            f\"The stock has also shown stellar quarterly returns of {row['Qtr Change %']}%, \"\n",
        "            f\"outperforming benchmarks.\"\n",
        "            if row.get('Qtr Change %', 0) > 15 else \"\"\n",
        "        )\n",
        "\n",
        "        stellar_relative_performance = (\n",
        "            f\"The stock also delivered stellar relative returns compared to Nifty500 this month ({row['Relative returns vs Nifty500 month%']}%) \"\n",
        "            f\"and this quarter ({row['Relative returns vs Nifty500 quarter%']}%).\"\n",
        "            if row.get('Relative returns vs Nifty500 month%', 0) > 15\n",
        "            and row.get('Relative returns vs Nifty500 quarter%', 0) > 15\n",
        "            else \"\"\n",
        "        )\n",
        "\n",
        "        insider_trading_commentary = (\n",
        "            f\"Additionally, there have been significant insider trading and SAST buys last quarter, \"\n",
        "            f\"with more than 100,000 shares traded, indicating strong confidence in the stock's potential.\"\n",
        "            if row.get('Insider & SAST Buys Last Quarter', 0) > 100000 else \"\"\n",
        "        )\n",
        "\n",
        "        ath_commentary = (\n",
        "            f\"The stock hit a new all-time high today at ₹{row['Day High']}, breaking the previous 52-week high last recorded on {row['Date of 52W High']}.\"\n",
        "            if row.get('NewATH', False) else \"\"\n",
        "        )\n",
        "\n",
        "        # Combine all notable performance metrics\n",
        "        notable_performance = \" \".join(filter(None, [\n",
        "            stellar_quarterly_performance,\n",
        "            stellar_relative_performance,\n",
        "            insider_trading_commentary,\n",
        "            ath_commentary\n",
        "        ]))\n",
        "\n",
        "        # Construct the query\n",
        "        query = (\n",
        "            f\"Generate a summary for the following stock data:\\n\\n\"\n",
        "            f\"- Stock: {row.get('Stock', 'Unknown')}\\n\"\n",
        "            f\"- Day change: {row.get('Day change %', 0):.1f}%\\n\"\n",
        "            f\"- Closing price: ₹{row.get('Current Price', 'N/A')}\\n\"\n",
        "            f\"- Daily turnover: ₹{row.get('DailyTurnInCr', 'N/A')} Cr\\n\"\n",
        "            f\"- Trading volume multiple of the week: {row.get('Day volume multiple of week', 'N/A')}\\n\"\n",
        "            f\"- Percent of float traded on the primary exchange: {row.get('PercentFloatTradedPrimaryExchange', 0):.1f}%\\n\"\n",
        "            f\"- Revenue QoQ Growth: {row.get('Revenue QoQ Growth %', 0):.1f}%\\n\"\n",
        "            f\"- Basic EPS QoQ Growth: {row.get('Basic EPS QoQ Growth %', 0):.1f}%\\n\"\n",
        "            f\"- Day high: ₹{row.get('Day High', 'N/A')}\\n\"\n",
        "            f\"- 10-year high: ₹{row.get('10Yr High', 'N/A')}\\n\"\n",
        "            f\"- New All-Time High: {'Yes' if row.get('NewATH', False) else 'No'}\\n\"\n",
        "            f\"- Date of previous 52-week high: {row.get('Date of 52W High', 'N/A')}\\n\"\n",
        "            f\"- Piotroski Score: {row.get('Piotroski Score', 'N/A')}\\n\\n\"\n",
        "            f\"{notable_performance}\\n\\n\"\n",
        "            f\"Generate a friendly and engaging summary focusing on today's performance, recent growth, and notable highs.\\n\\n\"\n",
        "            f\"Remove any text containing '**' or markdown-style formatting.\"\n",
        "        )\n",
        "\n",
        "        # Retry mechanism\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                # Construct the payload for Perplexity's API\n",
        "                payload = {\n",
        "                    \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "                    \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
        "                }\n",
        "\n",
        "                # Make the API call\n",
        "                response = requests.post(\n",
        "                    \"https://api.perplexity.ai/chat/completions\",\n",
        "                    json=payload,\n",
        "                    headers={\"Authorization\": f\"Bearer {self.perplexity_api_key}\", \"Content-Type\": \"application/json\"}\n",
        "                )\n",
        "\n",
        "                # Raise an exception if the response status code is not 200\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Extract the summary content\n",
        "                summary = response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No summary generated.\")\n",
        "                logger.info(f\"Perplexity summary generated for {row.get('Stock', 'Unknown')}.\")\n",
        "                return summary\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Attempt {attempt + 1}/{max_retries} - Error generating summary for {row.get('Stock', 'Unknown')}: {e}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    return f\"Error generating summary after {max_retries} attempts: {e}\"\n",
        "\n",
        "\n",
        "    async def fetch_perplexity(self, session, query, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Fetches data from Perplexity API with retry logic.\n",
        "\n",
        "        Args:\n",
        "            session: The aiohttp session for the request.\n",
        "            query: The query to send to the Perplexity API.\n",
        "            retries: Number of retries in case of failure.\n",
        "            retry_delay: Delay in seconds between retries.\n",
        "\n",
        "        Returns:\n",
        "            dict: The response from the Perplexity API or an error message.\n",
        "        \"\"\"\n",
        "        api_url = \"https://api.perplexity.ai/chat/completions\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
        "        }\n",
        "\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                async with session.post(api_url, headers=headers, json=payload) as response:\n",
        "                    if response.status == 200:\n",
        "                        response_data = await response.json()\n",
        "                        logger.info(f\"Perplexity API response received for query: {query}\")\n",
        "                        return response_data\n",
        "                    else:\n",
        "                        logger.error(f\"Perplexity API error: HTTP {response.status}\")\n",
        "                        if attempt < retries - 1:\n",
        "                            logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                            await asyncio.sleep(retry_delay)\n",
        "                        else:\n",
        "                            return {\"error\": f\"HTTP {response.status}\"}\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error querying Perplexity API: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    await asyncio.sleep(retry_delay)\n",
        "                else:\n",
        "                    return {\"error\": str(e)}\n",
        "\n",
        "    def fetch_perplexity_results(self, queries, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Fetches Perplexity news results for the given queries with retry logic.\n",
        "\n",
        "        Args:\n",
        "            queries: List of queries to fetch results for.\n",
        "            retries: Number of retries in case of failure.\n",
        "            retry_delay: Delay in seconds between retries.\n",
        "\n",
        "        Returns:\n",
        "            list: List of responses for each query.\n",
        "        \"\"\"\n",
        "        async def fetch_all():\n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                tasks = [\n",
        "                    self.fetch_perplexity(session, query, retries=retries, retry_delay=retry_delay)\n",
        "                    for query in queries\n",
        "                ]\n",
        "                return await asyncio.gather(*tasks)\n",
        "\n",
        "        return asyncio.run(fetch_all())\n",
        "\n",
        "    async def fetch_all_news(self, queries, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Fetches all news results using Perplexity API with retry logic.\n",
        "\n",
        "        Args:\n",
        "            queries: List of queries to fetch results for.\n",
        "            retries: Number of retries in case of failure.\n",
        "            retry_delay: Delay in seconds between retries.\n",
        "\n",
        "        Returns:\n",
        "            list: List of responses for each query.\n",
        "        \"\"\"\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = [\n",
        "                self.fetch_perplexity(session, query, retries=retries, retry_delay=retry_delay)\n",
        "                for query in queries\n",
        "            ]\n",
        "            return await asyncio.gather(*tasks)\n",
        "\n",
        "\n",
        "\n",
        "    def combine_summaries(self, row):\n",
        "        \"\"\"\n",
        "        Combine Perplexity-generated summaries into a single, coherent, and engaging news byte.\n",
        "        Handles potential errors and ensures valid payloads for API requests.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Define market hours and determine market status\n",
        "            ist = timezone('Asia/Kolkata')\n",
        "            current_time = datetime.now(ist)\n",
        "            market_close_time = current_time.replace(hour=15, minute=30, second=0, microsecond=0)\n",
        "\n",
        "            market_status_message = (\n",
        "                \"This commentary is being generated while the market is open. \"\n",
        "                \"Let's take a recent look at today's price action and performance.\"\n",
        "                if current_time < market_close_time else\n",
        "                \"This commentary is being generated after the market has closed. \"\n",
        "                \"Here's a summary of the end-of-day performance and updates.\"\n",
        "            )\n",
        "\n",
        "            # Validate and clean up data for API payload\n",
        "            stock = row.get('Stock', 'Unknown Stock')\n",
        "            day_change = f\"{row.get('Day change %', 'N/A')}%\"\n",
        "            contextual_summary = row.get('ContextualSummary', 'No summary available.').strip()\n",
        "            perplexity_news = row.get('PerplexityNews', 'No news available.').strip()\n",
        "\n",
        "            # Refine and truncate the summaries to fit API limitations (e.g., max characters)\n",
        "            contextual_summary = contextual_summary[:1000] + \"...\" if len(contextual_summary) > 1000 else contextual_summary\n",
        "            perplexity_news = perplexity_news[:1000] + \"...\" if len(perplexity_news) > 1000 else perplexity_news\n",
        "\n",
        "            # Construct the input for Perplexity\n",
        "            combined_input = (\n",
        "                f\"{market_status_message}\\n\\n\"\n",
        "                f\"Today's top performer is {stock}, with a daily change of {day_change}.\\n\\n\"\n",
        "                f\"Contextual Summary:\\n{contextual_summary}\\n\\n\"\n",
        "                f\"Perplexity News:\\n{perplexity_news}\\n\\n\"\n",
        "                f\"Generate a single, engaging, and conversational news byte. \"\n",
        "                f\"Highlight the daily percentage change, include updates from Perplexity News, and ensure all critical details are retained. \"\n",
        "                f\"Convert the ₹ symbol to 'rupees', round all numbers to 1 decimal place, and maintain a video-friendly tone.\\n\\n\"\n",
        "                f\"Remove any text containing '**' or markdown-style formatting.\\n\\n\"\n",
        "            )\n",
        "\n",
        "            # Use Perplexity to generate the combined summary\n",
        "            payload = {\n",
        "                \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": combined_input}]\n",
        "            }\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "\n",
        "            # API Call\n",
        "            api_url = \"https://api.perplexity.ai/chat/completions\"\n",
        "            response = requests.post(api_url, json=payload, headers=headers)\n",
        "\n",
        "            # Handle response\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                summary = result.get('choices', [{}])[0].get('message', {}).get('content', \"No summary generated.\")\n",
        "                logger.info(f\"Combined summary successfully generated for {stock}.\")\n",
        "                return summary.strip()\n",
        "\n",
        "            # Log detailed error information for debugging\n",
        "            else:\n",
        "                logger.error(f\"Error combining summaries for {stock}: {response.status_code} - {response.text}\")\n",
        "                return f\"Error combining summaries for {stock}: {response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Exception combining summaries for {row.get('Stock', 'Unknown Stock')}: {str(e)}\")\n",
        "            return f\"Error combining summaries: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "    def generate_initial_news_byte(self, df, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Generates the initial news byte from combined summaries and daily changes with retry logic.\n",
        "        \"\"\"\n",
        "        # Prepare the stock_changes and market_status_message dynamically\n",
        "        stock_changes = \"\\n\".join(\n",
        "            f\"{row['Stock']}: {row['Day change %']}%\" for _, row in df.iterrows()\n",
        "        )\n",
        "        logger.debug(f\"Stock changes prepared: {stock_changes}\")\n",
        "\n",
        "        ist = timezone(\"Asia/Kolkata\")\n",
        "        current_time = datetime.now(ist)\n",
        "        market_close_time = current_time.replace(hour=15, minute=30, second=0, microsecond=0)\n",
        "\n",
        "        market_status_message = (\n",
        "            \"This commentary is being generated while the market is open. \"\n",
        "            \"Here's a quick overview of the current market performance.\"\n",
        "            if current_time < market_close_time else\n",
        "            \"This commentary is being generated after the market has closed. \"\n",
        "            \"Here's a summary of the day's top gainers and updates.\"\n",
        "        )\n",
        "        logger.debug(f\"Market status message prepared: {market_status_message}\")\n",
        "\n",
        "        combined_input = (\n",
        "            f\"Today's top gainers and their daily percentage changes are as follows:\\n\\n{stock_changes}\\n\\n\"\n",
        "            f\"{market_status_message}\\n\\n\"\n",
        "            + \"\\n\".join(f\"- {summary}\" for summary in df['CombinedSummary'])\n",
        "            + \"\\n\\nGenerate a single, coherent, and engaging news byte that starts by summarizing the top gainers \"\n",
        "            \"and then includes the detailed updates for each stock. Highlight any notable performance metrics such as quarterly revenue growth or EPS growth, \"\n",
        "            \"and include any price-impacting news like earnings announcements, major deals, or market events that led to today's exceptional performance. \"\n",
        "            \"Ensure the language is conversational, easy to understand, and all numbers and percentages are rounded to 1 decimal place. \"\n",
        "            \"Remove any text containing '**' or markdown-style formatting.\\n\\n\"\n",
        "            \"End with a reminder to stay tuned to MoneyPulse.ai for more updates.\\n\\n\"\n",
        "            \"Make sure that in case a '-' is present between two numbers for instance 80-100, convert the '-' to 'to'. \"\n",
        "            \"Convert the ₹ symbol to 'rupees'. \"\n",
        "            \"Make sure to convert Ltd. to Limited. \"\n",
        "            \"The entire response must be strictly limited to 2000 words or less.\"\n",
        "        )\n",
        "\n",
        "        payload = {\n",
        "            \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": combined_input}],\n",
        "            \"max_tokens\": 1500  # Approximate token limit for 2000 words\n",
        "        }\n",
        "\n",
        "        # Retry logic\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = requests.post(self.api_url, json=payload, headers=self.headers)\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    news_byte = result.get('choices', [{}])[0].get('message', {}).get('content', \"No news byte generated.\")\n",
        "\n",
        "                    # Clean the generated news byte\n",
        "                    cleaned_news_byte = re.sub(r\"\\*\\*.*?\\*\\*\", \"\", news_byte).strip()\n",
        "                    logger.info(\"Initial news byte generated successfully.\")\n",
        "                    return cleaned_news_byte\n",
        "                else:\n",
        "                    logger.error(f\"Perplexity API error: {response.status_code} - {response.text}\")\n",
        "                    if attempt < retries - 1:\n",
        "                        logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                        time.sleep(retry_delay)\n",
        "                    else:\n",
        "                        return f\"Error generating initial news byte: {response.status_code}\"\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Exception generating initial news byte: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    return f\"Error generating initial news byte: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "    def shorten_news_byte(self, initial_news_byte, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Shortens the news byte if it exceeds 2000 words, with retry logic.\n",
        "        \"\"\"\n",
        "        shorten_request = (\n",
        "            \"The previous response exceeded 2000 words. Please provide a shorter version \"\n",
        "            \"of the following news byte, strictly under 2000 words while retaining all key information:\\n\\n\"\n",
        "            f\"{initial_news_byte}\\n\\n\"\n",
        "            \"Ensure the shortened version is coherent, engaging, and includes all important updates.\"\n",
        "        )\n",
        "\n",
        "        payload = {\n",
        "            \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": shorten_request}],\n",
        "            \"max_tokens\": 1500\n",
        "        }\n",
        "\n",
        "        # Retry logic\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = requests.post(self.api_url, json=payload, headers=self.headers)\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    news_byte = result.get('choices', [{}])[0].get('message', {}).get('content', \"No news byte generated.\")\n",
        "\n",
        "                    # Clean the shortened news byte\n",
        "                    cleaned_news_byte = re.sub(r\"\\*\\*.*?\\*\\*\", \"\", news_byte).strip()\n",
        "                    logger.info(\"Shortened news byte generated successfully.\")\n",
        "                    return cleaned_news_byte\n",
        "                else:\n",
        "                    logger.error(f\"Perplexity API error: {response.status_code} - {response.text}\")\n",
        "                    if attempt < retries - 1:\n",
        "                        logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                        time.sleep(retry_delay)\n",
        "                    else:\n",
        "                        return f\"Error shortening news byte: {response.status_code}\"\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Exception shortening news byte: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    return f\"Error shortening news byte: {e}\"\n",
        "\n",
        "\n",
        "    def generate_final_news_byte(self, df):\n",
        "        \"\"\"\n",
        "        Generates the final news byte, shortening if necessary.\n",
        "        Ensures retries in case of errors and provides comprehensive logging.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Generate the initial news byte\n",
        "            logger.info(\"Starting to generate the initial news byte.\")\n",
        "            initial_news_byte = self.generate_initial_news_byte(df)\n",
        "\n",
        "            if not initial_news_byte or \"Error\" in initial_news_byte:\n",
        "                logger.error(\"Failed to generate the initial news byte.\")\n",
        "                return \"Error generating the initial news byte.\"\n",
        "\n",
        "            # Check the word count of the initial news byte\n",
        "            word_count = len(initial_news_byte.split())\n",
        "            logger.info(f\"Initial news byte generated with {word_count} words.\")\n",
        "\n",
        "            # If within the word limit, return the initial news byte\n",
        "            if word_count <= 1000:\n",
        "                logger.info(\"Initial news byte is within the word limit. Returning the final news byte.\")\n",
        "                return initial_news_byte\n",
        "\n",
        "            # Log and attempt to shorten the news byte\n",
        "            logger.warning(f\"Initial news byte exceeded 2000 words ({word_count}). Attempting to shorten.\")\n",
        "            retries = 3\n",
        "            shortened_news_byte = None\n",
        "\n",
        "            while retries > 0:\n",
        "                try:\n",
        "                    shortened_news_byte = self.shorten_news_byte(initial_news_byte)\n",
        "                    final_word_count = len(shortened_news_byte.split())\n",
        "                    logger.info(f\"Shortened news byte generated with {final_word_count} words.\")\n",
        "\n",
        "                    # If the shortened news byte is within the limit, return it\n",
        "                    if final_word_count <= 2000:\n",
        "                        logger.info(\"Shortened news byte is within the word limit. Returning the final news byte.\")\n",
        "                        return shortened_news_byte\n",
        "                except Exception as e:\n",
        "                    retries -= 1\n",
        "                    logger.error(f\"Error shortening the news byte. Retries left: {retries}. Error: {e}\")\n",
        "\n",
        "            # If retries are exhausted, return the initial news byte\n",
        "            logger.warning(\"Failed to shorten the news byte within the word limit. Returning the original news byte.\")\n",
        "            return initial_news_byte\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error in generating the final news byte: {e}\")\n",
        "            return \"Error generating the final news byte.\"\n",
        "\n",
        "\n",
        "\n",
        "    def upload_image_to_s3(self, file_path, title):\n",
        "        \"\"\"\n",
        "        Uploads an image file to an S3 bucket.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            aws_access_key_id = userdata.get(\"aws_access_key_id\")\n",
        "            aws_secret_access_key = userdata.get(\"aws_secret_access_key\")\n",
        "\n",
        "            if not aws_access_key_id or not aws_secret_access_key:\n",
        "                raise Exception(\"AWS credentials are not set in Colab userdata.\")\n",
        "\n",
        "            # Create an S3 client\n",
        "            s3 = boto3.client(\n",
        "                \"s3\",\n",
        "                aws_access_key_id=aws_access_key_id,\n",
        "                aws_secret_access_key=aws_secret_access_key,\n",
        "                region_name=\"ap-south-1\",\n",
        "            )\n",
        "\n",
        "            # Generate the S3 file key for the image file\n",
        "            file_key = f\"image/{title.replace(' ', '_').lower()}.png\"\n",
        "\n",
        "            # Upload the image file to the S3 bucket\n",
        "            s3.upload_file(file_path, \"finbytes\", file_key)\n",
        "\n",
        "            # Construct the public URL for the uploaded file\n",
        "            file_url = f\"https://finbytes.s3.ap-south-1.amazonaws.com/{file_key}\"\n",
        "            return file_url\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to upload image to S3: {str(e)}\")\n",
        "\n",
        "\n",
        "    def generate_plot(self, df):\n",
        "        try:\n",
        "            # Set the aesthetic style of the plots\n",
        "            sns.set_theme(style=\"ticks\")  # Use 'ticks' to enable control of spines\n",
        "\n",
        "            # Define the specific green color\n",
        "            green_color = ['#008000']  # Use a single green color\n",
        "\n",
        "            # Wrap long stock names\n",
        "            df['Stock'] = df['Stock'].apply(lambda x: fill(x, width=15))\n",
        "\n",
        "            # Create the figure with mobile-friendly dimensions\n",
        "            plt.figure(figsize=(6, 10))  # Adjust aspect ratio (width x height)\n",
        "\n",
        "            # Create the horizontal bar plot\n",
        "            bars = sns.barplot(\n",
        "                x='Day change %',\n",
        "                y='Stock',\n",
        "                data=df,\n",
        "                color=green_color[0],  # Single color used for the entire bar plot\n",
        "                orient='h'\n",
        "            )\n",
        "\n",
        "            # Add data labels on the bars\n",
        "            for bar in bars.patches:\n",
        "                bars.annotate(\n",
        "                    f\"{bar.get_width():.1f}%\",  # Format to 1 decimal place\n",
        "                    (bar.get_width() + 0.2, bar.get_y() + bar.get_height() / 2),  # Position the text\n",
        "                    ha='left', va='center', fontsize=10, fontweight='bold', color='white', xytext=(5, 0),\n",
        "                    textcoords='offset points'\n",
        "                )\n",
        "\n",
        "            # Remove gridlines and retain only left and bottom axes\n",
        "            sns.despine(left=False, bottom=False)  # Retain left and bottom borders\n",
        "            plt.gca().grid(False)  # Turn off all gridlines\n",
        "\n",
        "            # Set the main title\n",
        "            plt.title('Daily % Gain Of Top Gainers', fontsize=16, fontweight='bold', pad=15, color='white')\n",
        "\n",
        "            # Set axis labels\n",
        "            plt.xlabel('Daily % Gain', fontsize=12, fontweight='bold', color='white')\n",
        "            plt.ylabel('Stocks', fontsize=12, fontweight='bold', color='white')\n",
        "\n",
        "            # Adjust font size and make stock names bold\n",
        "            plt.xticks(fontsize=10, fontweight='bold', color='white')\n",
        "            plt.yticks(fontsize=10, fontweight='bold', ha='right', color='white')  # Align stock names to the left\n",
        "\n",
        "            # Set background color to black\n",
        "            plt.gca().set_facecolor('black')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save the plot\n",
        "            plt.savefig(self.plot_path, dpi=200, facecolor='black')  # Ensure the background is saved as black\n",
        "            plt.close()\n",
        "\n",
        "            logger.info(f\"Mobile-friendly horizontal bar plot saved to {self.plot_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating horizontal bar plot: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_summary_as_json(self, final_news_byte, plot_url):\n",
        "        \"\"\"\n",
        "        Save the final combined news byte and metadata as a JSON file.\n",
        "        \"\"\"\n",
        "        # Get current time in IST (UTC+5:30)\n",
        "        ist = timezone(\"Asia/Kolkata\")\n",
        "        created_time = datetime.now(ist).isoformat()  # Generate ISO format with timezone\n",
        "\n",
        "        # JSON output\n",
        "        output = {\n",
        "            \"created_time\": created_time,  # Use IST with UTC+5:30 offset\n",
        "            \"title\": \"Top Gainers Today\",\n",
        "            \"summary_text\": final_news_byte,\n",
        "            \"image_url\": plot_url,  # Include the uploaded plot URL\n",
        "        }\n",
        "\n",
        "        # Save the JSON to a file\n",
        "        json_path = \"/content/TopGainers_summary.json\"\n",
        "        try:\n",
        "            with open(json_path, \"w\") as f:\n",
        "                json.dump(output, f, indent=4)\n",
        "            logger.info(f\"JSON summary saved to {json_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving JSON summary to {json_path}: {e}\")\n",
        "\n",
        "\n",
        "    def post_json_to_summary(self, json_data):\n",
        "        \"\"\"\n",
        "        Posts a JSON summary to the specified API endpoint.\n",
        "\n",
        "        Args:\n",
        "            json_data (dict): The JSON data to be posted to the API.\n",
        "\n",
        "        Returns:\n",
        "            dict: The response from the API if successful.\n",
        "\n",
        "        Raises:\n",
        "            Exception: If the POST request fails.\n",
        "        \"\"\"\n",
        "        api_url = \"https://sy6foef31c.execute-api.ap-south-1.amazonaws.com/default/create\"\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "        try:\n",
        "            response = requests.post(api_url, json=json_data, headers=headers)\n",
        "            response.raise_for_status()  # Raise an error for non-2xx responses\n",
        "            logger.info(f\"POST request successful. Response: {response.json()}\")\n",
        "            return response.json()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error posting JSON to API: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Main method to execute the TopGainers process.\"\"\"\n",
        "        logger.info(\"Starting the TopGainers process.\")\n",
        "\n",
        "        # Step 1: Read and preprocess the input CSV\n",
        "        try:\n",
        "            df = pd.read_csv(self.input_path)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"Input file not found at {self.input_path}.\")\n",
        "            return\n",
        "\n",
        "        # Process top 4 stocks based on \"Day change %\"\n",
        "        if len(df) < 4:\n",
        "            logger.warning(\"Dataset has fewer than 4 stocks. Adjusting to available rows.\")\n",
        "        else:\n",
        "            df = df.sort_values(by=\"Day change %\", ascending=False).head(4)\n",
        "\n",
        "        # Step 2: Generate ChatGPT summaries\n",
        "        logger.info(\"Generating Contextual summaries.\")\n",
        "        try:\n",
        "            df['ContextualSummary'] = df.apply(self.generate_contextual_summary, axis=1)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating contextual summaries: {e}\")\n",
        "            df['ContextualSummary'] = [\"Error generating summary\" for _ in range(len(df))]\n",
        "\n",
        "        # Step 3: Fetch Perplexity news\n",
        "        logger.info(\"Fetching news from Perplexity.\")\n",
        "        try:\n",
        "            perplexity_queries = [\n",
        "                f\"Find recent news and updates about {row['Stock']} in the last week.\" for _, row in df.iterrows()\n",
        "            ]\n",
        "            perplexity_results = self.fetch_perplexity_results(perplexity_queries)\n",
        "            df['PerplexityNews'] = [\n",
        "                res.get(\"choices\", [{\"message\": {\"content\": \"No recent news found.\"}}])[0][\"message\"][\"content\"]\n",
        "                if isinstance(res, dict) and \"error\" not in res else \"Error occurred\"\n",
        "                for res in perplexity_results\n",
        "            ]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching news from Perplexity: {e}\")\n",
        "            df['PerplexityNews'] = [\"Error fetching news\" for _ in range(len(df))]\n",
        "\n",
        "        # Step 4: Combine ChatGPT summaries with Perplexity news\n",
        "        logger.info(\"Combining ChatGPT summaries and Perplexity news.\")\n",
        "        try:\n",
        "            df['CombinedSummary'] = df.apply(self.combine_summaries, axis=1)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error combining summaries: {e}\")\n",
        "            df['CombinedSummary'] = [\"Error combining summaries\" for _ in range(len(df))]\n",
        "\n",
        "        # Step 5: Generate a single final news byte covering all stocks\n",
        "        logger.info(\"Generating final news byte for all stocks.\")\n",
        "        try:\n",
        "            final_news_byte = self.generate_final_news_byte(df)  # Pass the DataFrame to the method\n",
        "            df['FinalNewsByte'] = [final_news_byte for _ in range(len(df))]  # Add the same final news byte for all rows\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating final news byte: {e}\")\n",
        "            final_news_byte = f\"Error generating final news byte: {e}\"\n",
        "            df['FinalNewsByte'] = [final_news_byte for _ in range(len(df))]\n",
        "\n",
        "        # Step 6: Save the plot and upload it to S3\n",
        "        logger.info(\"Generating and saving plot.\")\n",
        "        try:\n",
        "            self.generate_plot(df)\n",
        "            logger.info(\"Uploading the plot image to S3.\")\n",
        "            plot_url = self.upload_image_to_s3(self.plot_path, \"Top Gainers Today\")\n",
        "            logger.info(f\"Plot image successfully uploaded to S3. URL: {plot_url}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating or uploading plot: {e}\")\n",
        "            plot_url = None\n",
        "\n",
        "        # Step 7: Prepare JSON for API posting\n",
        "        json_data = {\n",
        "            \"created_time\": datetime.now(timezone('Asia/Kolkata')).isoformat(),\n",
        "            \"title\": \"Top Gainers Today\",\n",
        "            \"summary_text\": final_news_byte,\n",
        "            \"image_url\": plot_url\n",
        "        }\n",
        "\n",
        "        # # # Step 8: Post JSON data to API\n",
        "        # try:\n",
        "        #     logger.info(\"Posting JSON data to API.\")\n",
        "        #     self.post_json_to_summary(json_data)\n",
        "        # except Exception as e:\n",
        "        #     logger.error(f\"Error posting JSON data to API: {e}\")\n",
        "\n",
        "        # Step 9: Save the DataFrame to CSV\n",
        "        logger.info(\"Saving the final output CSV.\")\n",
        "        try:\n",
        "            df.to_csv(self.output_path, index=False)\n",
        "            logger.info(f\"Output saved to {self.output_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving the output CSV: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Run the class\n",
        "TopGainers().run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install boto3\n",
        "# !pip uninstall -y openai\n",
        "# !pip install openai\n",
        "# !pip uninstall -y httpx\n",
        "# !pip install httpx"
      ],
      "metadata": {
        "id": "M63zXt6T58Ug"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}